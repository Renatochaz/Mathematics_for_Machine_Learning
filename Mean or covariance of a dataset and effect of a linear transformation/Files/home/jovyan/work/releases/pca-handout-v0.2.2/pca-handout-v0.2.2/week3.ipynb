{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal Projections\n",
    "\n",
    "We will write functions that will implement orthogonal projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "1. Write code that projects data onto lower-dimensional subspaces.\n",
    "2. Understand the real world applications of projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we will first import the packages that we need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-180e011cfab68522",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT THIS CELL\r\n",
    "import numpy as np\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "plt.style.use('fivethirtyeight')\r\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will retrieve the Olivetti faces dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46370283380b8b3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\r\n",
    "from ipywidgets import interact\r\n",
    "image_shape = (64, 64)\r\n",
    "# Load faces data\r\n",
    "dataset = fetch_olivetti_faces('./')\r\n",
    "faces = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advice for testing numerical algorithms\n",
    "Before we begin this week's assignment, there are some advice that we would like to give for writing functions that work with numerical data. They are useful for finding bugs in your implementation.\n",
    "\n",
    "Testing machine learning algorithms (or numerical algorithms in general)\n",
    "is sometimes really hard as it depends on the dataset\n",
    "to produce an answer, and you will never be able to test your algorithm on all the datasets\n",
    "we have in the world. Nevertheless, we have some tips for you to help you identify bugs in\n",
    "your implementations.\n",
    "\n",
    "#### 1. Test on small dataset\n",
    "Test your algorithms on small dataset: datasets of size 1 or 2 sometimes will suffice. This\n",
    "is useful because you can (if necessary) compute the answers by hand and compare them with\n",
    "the answers produced by the computer program you wrote. In fact, these small datasets can even have special numbers,\n",
    "which will allow you to compute the answers by hand easily.\n",
    "\n",
    "#### 2. Find invariants\n",
    "Invariants refer to properties of your algorithm and functions that are maintained regardless\n",
    "of the input. We will highlight this point later in this notebook where you will see functions,\n",
    "which will check invariants for some of the answers you produce.\n",
    "\n",
    "Invariants you may want to look for:\n",
    "1. Does your algorithm always produce a positive/negative answer, or a positive definite matrix?\n",
    "2. If the algorithm is iterative, do the intermediate results increase/decrease monotonically?\n",
    "3. Does your solution relate with your input in some interesting way, e.g. orthogonality? \n",
    "\n",
    "Finding invariants is hard, and sometimes there simply isn't any invariant. However, DO take advantage of them if you can find them. They are the most powerful checks when you have them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find some invariants for projections. In the cell below, we have written two functions which check for invariants of projections. See the docstrings which explain what each of them does. You should use these functions to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d144d0ec1706c84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy.testing as np_test\r\n",
    "def test_property_projection_matrix(P):\r\n",
    "    \"\"\"Test if the projection matrix satisfies certain properties.\r\n",
    "    In particular, we should have P @ P = P, and P = P^T\r\n",
    "    \"\"\"\r\n",
    "    np_test.assert_almost_equal(P, P @ P)\r\n",
    "    np_test.assert_almost_equal(P, P.T)\r\n",
    "\r\n",
    "def test_property_projection(x, p):\r\n",
    "    \"\"\"Test orthogonality of x and its projection p.\"\"\"\r\n",
    "    np_test.assert_almost_equal(p.T @ (p-x), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Orthogonal Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for projection of a vector $\\boldsymbol x$ onto a 1-dimensional subspace $U$ with basis vector $\\boldsymbol b$ we have\n",
    "\n",
    "$${\\pi_U}(\\boldsymbol x) = \\frac{\\boldsymbol b\\boldsymbol b^T}{{\\lVert\\boldsymbol  b \\rVert}^2}\\boldsymbol x $$\n",
    "\n",
    "And for the general projection onto an M-dimensional subspace $U$ with basis vectors $\\boldsymbol b_1,\\dotsc, \\boldsymbol b_M$ we have\n",
    "\n",
    "$${\\pi_U}(\\boldsymbol x) = \\boldsymbol B(\\boldsymbol B^T\\boldsymbol B)^{-1}\\boldsymbol B^T\\boldsymbol x $$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\boldsymbol B = [\\boldsymbol b_1,...,\\boldsymbol b_M]$$\n",
    "\n",
    "\n",
    "Your task is to implement orthogonal projections. We can split this into two steps\n",
    "1. Find the projection matrix $\\boldsymbol P$ that projects any $\\boldsymbol x$ onto $U$.\n",
    "2. The projected vector $\\pi_U(\\boldsymbol x)$ of $\\boldsymbol x$ can then be written as $\\pi_U(\\boldsymbol x) = \\boldsymbol P\\boldsymbol x$.\n",
    "\n",
    "To perform step 1, you need to complete the function `projection_matrix_1d` and `projection_matrix_general`. To perform step 2, complete `project_1d` and `project_general`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection (1d)\r\n",
    "\r\n",
    "Recall that you can use `np.dot(a, b)` or `a@b` to perform matrix-matrix or matrix-vector multiplication. `a*b` shall compute the element-wise product of $\\boldsymbol a$ and $\\boldsymbol b$.\r\n",
    "\r\n",
    "You may find the function [np.outer()](https://numpy.org/doc/stable/reference/generated/numpy.outer.html) useful.\r\n",
    "\r\n",
    "Remember that the transpose operation does not do anything for 1 dimensional arrays. So you cannot compute $\\boldsymbol b\\boldsymbol b^T$ using `np.dot(b, b.T)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad3644c3c86c02dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\r\n",
    "def projection_matrix_1d(b):\r\n",
    "    \"\"\"Compute the projection matrix onto the space spanned by `b`\r\n",
    "    Args:\r\n",
    "        b: ndarray of dimension (D,), the basis for the subspace\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        P: the projection matrix\r\n",
    "    \"\"\"\r\n",
    "    # YOUR CODE HERE\n",
    "    ### Uncomment and modify the code below\r\n",
    "#     D, = b.shape\r\n",
    "#     ### Edit the code below to compute a projection matrix of shape (D,D)\r\n",
    "#     P = np.zeros((D, D)) # <-- EDIT THIS\r\n",
    "#     # You may be tempted to follow the formula and implement bb^T as b @ b.T in numpy.\r\n",
    "#     # However, notice that this b is a 1D ndarray, so b.T is an no-op. Use np.outer instead\r\n",
    "#     # to implement the outer product.\r\n",
    "#     return P "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the function `projection_matrix_1d`, you should be able \n",
    "to implement `project_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-db73798e7a056a80",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\r\n",
    "def project_1d(x, b):\r\n",
    "    \"\"\"Compute the projection matrix onto the space spanned by `b`\r\n",
    "    Args:\r\n",
    "        x: the vector to be projected\r\n",
    "        b: ndarray of dimension (D,), the basis for the subspace\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        y: ndarray of shape (D,) projection of x in space spanned by b\r\n",
    "    \"\"\"\r\n",
    "    # YOUR CODE HERE\n",
    "    ### Uncomment and modify the code below\r\n",
    "#     p = np.zeros((3,)) # <-- EDIT THIS\r\n",
    "#     return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-755c8afdeb75aa71",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test 1D\r\n",
    "# Test that we computed the correct projection matrix\r\n",
    "from numpy.testing import assert_allclose\r\n",
    "\r\n",
    "assert_allclose(\r\n",
    "    projection_matrix_1d(np.array([1, 2, 2])), \r\n",
    "    np.array([[1,  2,  2],\r\n",
    "              [2,  4,  4],\r\n",
    "              [2,  4,  4]]) / 9\r\n",
    ")\r\n",
    "\r\n",
    "# Some hidden tests below\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-439a0c8ed8e60c3b",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test that we project x on to the 1d subspace correctly\r\n",
    "assert_allclose(\r\n",
    "    project_1d(np.ones(3), np.array([1, 2, 2])),\r\n",
    "    np.array([5, 10, 10]) / 9\r\n",
    ")\r\n",
    "\r\n",
    "# Some hidden tests below\r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection (ND)\r\n",
    "\r\n",
    "You may find the function [np.linalg.inv()](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-171a4c99d0c00d94",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\r\n",
    "def projection_matrix_general(B):\r\n",
    "    \"\"\"Compute the projection matrix onto the space spanned by the columns of `B`\r\n",
    "    Args:\r\n",
    "        B: ndarray of dimension (D, M), the basis for the subspace\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        P: the projection matrix\r\n",
    "    \"\"\"\r\n",
    "    # YOUR CODE HERE\n",
    "    ### Uncomment and modify the code below\r\n",
    "#     P = np.eye(B.shape[0]) # <-- EDIT THIS\r\n",
    "#     return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-03ad12056ad6c317",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\r\n",
    "def project_general(x, B):\r\n",
    "    \"\"\"Compute the projection matrix onto the space spanned by the columns of `B`\r\n",
    "    Args:\r\n",
    "        x: ndarray of dimension (D, 1), the vector to be projected\r\n",
    "        B: ndarray of dimension (D, M), the basis for the subspace\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        p: projection of x onto the subspac spanned by the columns of B; size (D, 1)\r\n",
    "    \"\"\"\r\n",
    "    # YOUR CODE HERE\n",
    "    # Uncomment and modify the code below\r\n",
    "#     p = np.zeros(x.shape) # <-- EDIT THIS\r\n",
    "#     return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our discussion earlier about invariants? In the next cell, we will check that these invariants hold for the functions that you have implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1c7cc6a2b0ad3323",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_allclose\r\n",
    "\r\n",
    "B = np.array([[1, 0],\r\n",
    "              [1, 1],\r\n",
    "              [1, 2]])\r\n",
    "\r\n",
    "assert_allclose(\r\n",
    "    projection_matrix_general(B), \r\n",
    "    np.array([[5,  2, -1],\r\n",
    "              [2,  2,  2],\r\n",
    "              [-1, 2,  5]]) / 6\r\n",
    ")\r\n",
    "\r\n",
    "# Some hidden tests below\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cba786ceb5d23cbc",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test 2D\r\n",
    "# Test that we computed the correct projection matrix\r\n",
    "\r\n",
    "# Test that we project x on to the 2d subspace correctly\r\n",
    "assert_allclose(\r\n",
    "    project_general(np.array([6, 0, 0]).reshape(-1,1), B), \r\n",
    "    np.array([5, 2, -1]).reshape(-1,1)\r\n",
    ")\r\n",
    "\r\n",
    "# Some hidden tests below\r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Eigenfaces  (optional)\r\n",
    "\r\n",
    "Next, you will see what happens if you project a dataset consisting of human faces onto a subset of a basis we call the \"eigenfaces\". This technique of projecting the data onto a subspace with smaller dimension, and thus reducing the number of coordinates required to represent the data, is widely used in machine learning. It can help identify trends and relationships between variables that are otherwise hidden away.\r\n",
    "\r\n",
    "We have already prepared the eigenfaces basis using Principle Component Analysis (PCA), which finds the best possible basis to represent this smaller subspace. Next week you will derive PCA and implement the PCA algorithm, but for now you'll simply see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's import the packages that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\r\n",
    "from ipywidgets import interact\r\n",
    "%matplotlib inline\r\n",
    "image_shape = (64, 64)\r\n",
    "# Load faces data\r\n",
    "dataset = fetch_olivetti_faces(data_home='./')\r\n",
    "faces = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each face of the dataset is a gray scale image of size (64, 64). Let's visualize some faces in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\r\n",
    "plt.imshow(np.hstack(faces[:5].reshape(5,64,64)), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numerical reasons which you shall see in week 4, we normalize the dataset\r\n",
    "mean = faces.mean(axis=0)\r\n",
    "std = faces.std(axis=0)\r\n",
    "faces_normalized = (faces - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the basis has been saved in a file named `eigenfaces.npy`, first we load it into the variable B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.load('eigenfaces.npy')[:50] # we use the first 50 basis vectors --- you should play around with this.\r\n",
    "print(\"the eigenfaces have shape {}\".format(B.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance in $\\boldsymbol B$ is a `64x64' image, an \"eigenface\", which we determined using an algorithm called Principal Component Analysis. Let's visualize \n",
    "a few of those \"eigenfaces\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\r\n",
    "plt.imshow(np.hstack(B[:5].reshape(-1, 64, 64)), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at what happens if we project our faces onto the basis $\\boldsymbol B$ spanned by these 50 \"eigenfaces\". In order to do this, we need to reshape $\\boldsymbol B$ from above, which is of size (50, 64, 64), into the same shape as the matrix representing the basis as we have done earlier, which is of size (4096, 50). Here 4096 is the dimensionality of the data and 50 is the number of data points. \n",
    "\n",
    "Then we can reuse the functions we implemented earlier to compute the projection matrix and the projection. Complete the code below to visualize the reconstructed faces that lie on the subspace spanned by the \"eigenfaces\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(i=(0, 10))\r\n",
    "def show_face_face_reconstruction(i):\r\n",
    "    original_face = faces_normalized[i].reshape(64, 64)\r\n",
    "    # reshape the data we loaded in variable `B` \r\n",
    "    B_basis = B.reshape(B.shape[0], -1).T\r\n",
    "    face_reconstruction = project_general(faces_normalized[i], B_basis).reshape(64, 64)\r\n",
    "    plt.figure()\r\n",
    "    plt.imshow(np.hstack([original_face, face_reconstruction]), cmap='gray')\r\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen to the reconstruction as we increase the dimension of our basis? \n",
    "\n",
    "Modify the code above to visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above you used a specially selected basis for your projections. What happens if you simply use a random basis instead?\r\n",
    "\r\n",
    "Below, you will project the data onto 50 randomly generated basis vectors. Before you run the code cells below, can you predict the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_random = np.random.randn(*B.shape)\r\n",
    "print(\"the random basis has shape {}\".format(B_random.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we shall visualize the \"faces\" represented by the basis vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\r\n",
    "plt.imshow(np.hstack(B_random[:5].reshape(-1, 64, 64)), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the basis vectors do not store faces but only store random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to project the faces onto this basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(i=(0, 10))\r\n",
    "def show_face_face_reconstruction(i):\r\n",
    "    original_face = faces_normalized[i].reshape(64, 64)\r\n",
    "    # reshape the data we loaded in variable `B` \r\n",
    "    B_basis = B_random.reshape(B_random.shape[0], -1).T\r\n",
    "    face_reconstruction = project_general(faces_normalized[i], B_basis).reshape(64, 64)\r\n",
    "    plt.figure()\r\n",
    "    plt.imshow(np.hstack([original_face, face_reconstruction]), cmap='gray')\r\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were these the results you expected? You can see how important it is to use a technique like PCA when selecting your basis for projection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Least squares regression (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we shall apply the concept of projection to finding the optimal parameters of a least squares regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case where we have a linear model for predicting housing prices. We are predicting the housing prices based on features in the \r\n",
    "housing dataset. If we denote the features as $\\boldsymbol x_0, \\dotsc, \\boldsymbol x_n$ and collect them into a vector $\\boldsymbol {x}$, and the price of the houses as $y$. Assume that we have \r\n",
    "a prediction model in the way such that $\\hat{y}_i =  f(\\boldsymbol {x}_i) = \\boldsymbol \\theta^T\\boldsymbol {x}_i$.\r\n",
    "\r\n",
    "\r\n",
    "If we collect the dataset into a $(N,D)$ data matrix $\\boldsymbol X$ (where $N$ is the number of houses and $D$ is the number of features for each house), we can write down our model like this:\r\n",
    "\r\n",
    "$$\r\n",
    "\\begin{bmatrix} \r\n",
    "\\boldsymbol{x}_1^T \\\\\r\n",
    "\\vdots \\\\ \r\n",
    "\\boldsymbol{x}_N^T \r\n",
    "\\end{bmatrix} \\boldsymbol{\\theta} = \\begin{bmatrix} \r\n",
    "y_1 \\\\\r\n",
    "\\vdots \\\\ \r\n",
    "y_2 \r\n",
    "\\end{bmatrix},\r\n",
    "$$\r\n",
    "\r\n",
    "i.e.,\r\n",
    "\r\n",
    "$$\r\n",
    "\\boldsymbol X\\boldsymbol{\\theta} = \\boldsymbol{y}.\r\n",
    "$$\r\n",
    "\r\n",
    "Note that the data points are the *rows* of the data matrix, i.e., every column is a dimension of the data. \r\n",
    "\r\n",
    "Our goal is to find the best $\\boldsymbol\\theta$ such that we minimize the following objective (least square).\r\n",
    "\r\n",
    "$$\r\n",
    "\\begin{eqnarray} \r\n",
    "& \\sum^n_{i=1}{\\lVert \\bar{y_i} - y_i \\rVert^2} \\\\\r\n",
    "&= \\sum^n_{i=1}{\\lVert \\boldsymbol \\theta^T\\boldsymbol{x}_i - y_i \\rVert^2} \\\\\r\n",
    "&= (\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y)^T(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y).\r\n",
    "\\end{eqnarray}\r\n",
    "$$\r\n",
    "\r\n",
    "If we set the gradient of the above objective to $\\boldsymbol  0$, we have\r\n",
    "$$\r\n",
    "\\begin{eqnarray} \r\n",
    "\\nabla_\\theta(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y)^T(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y) &=& \\boldsymbol 0 \\\\\r\n",
    "\\nabla_\\theta(\\boldsymbol {\\theta}^T\\boldsymbol X^T - \\boldsymbol y^T)(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y) &=& \\boldsymbol 0 \\\\\r\n",
    "\\nabla_\\theta(\\boldsymbol {\\theta}^T\\boldsymbol X^T\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y^T\\boldsymbol X\\boldsymbol \\theta - \\boldsymbol \\theta^T\\boldsymbol X^T\\boldsymbol y + \\boldsymbol y^T\\boldsymbol y ) &=& \\boldsymbol 0 \\\\\r\n",
    "2\\boldsymbol X^T\\boldsymbol X\\theta - 2\\boldsymbol X^T\\boldsymbol y &=& \\boldsymbol 0 \\\\\r\n",
    "\\boldsymbol X^T\\boldsymbol X\\boldsymbol \\theta        &=& \\boldsymbol X^T\\boldsymbol y.\r\n",
    "\\end{eqnarray}\r\n",
    "$$\r\n",
    "\r\n",
    "The solution that gives zero gradient solves (which we call the maximum likelihood estimator) the following equation:\r\n",
    "\r\n",
    "$$\\boldsymbol X^T\\boldsymbol X\\boldsymbol \\theta = \\boldsymbol X^T\\boldsymbol y.$$\r\n",
    "\r\n",
    "_This is exactly the same as the normal equation we have for projections_.\r\n",
    "\r\n",
    "This means that if we solve for $\\boldsymbol X^T\\boldsymbol X\\boldsymbol \\theta = \\boldsymbol X^T\\boldsymbol y.$ we would find the best $\\boldsymbol \\theta = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y$, i.e. the $\\boldsymbol \\theta$ which minimizes our objective.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put things into perspective. Consider that we want to predict the true coefficient $\\boldsymbol \\theta$ \r\n",
    "of the line $\\boldsymbol y = \\boldsymbol \\theta^T \\boldsymbol x$ given only $\\boldsymbol X$ and $\\boldsymbol y$. We do not know the true value of $\\boldsymbol \\theta$.\r\n",
    "\r\n",
    "Below, in a two dimensional plane, we shall generate 50 points on a line passing through the origin and with $\\boldsymbol \\theta$ (which is slope in this case) = 2. Then, we shall add some noise to all the points so that all the points do not end up being on the same line (if all the points are on the same line, it would make finding $\\boldsymbol \\theta$ extremely easy).\r\n",
    "\r\n",
    "Note: In this particular example, $\\boldsymbol \\theta$ is a scalar. Still, we can represent it as an $\\mathbb{R}^1$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, num=50)\r\n",
    "theta = 2\r\n",
    "def f(x):    \r\n",
    "    random = np.random.RandomState(42) # we use the same random seed so we get deterministic output\r\n",
    "    return theta * x + random.normal(scale=1.0, size=len(x)) # our observations are corrupted by some noise, so that we do not get (x,y) on a line\r\n",
    "\r\n",
    "y = f(x)\r\n",
    "plt.scatter(x, y);\r\n",
    "plt.xlabel('x');\r\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shall calculate $\\hat{\\boldsymbol  \\theta}$ using the formula which we derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1,1) # size N x 1\r\n",
    "Y = y.reshape(-1,1) # size N x 1\r\n",
    "\r\n",
    "# maximum likelihood estimator\r\n",
    "theta_hat = np.linalg.inv(X.T @ X) @ X.T @ Y\r\n",
    "print('Inferred slope =', theta_hat[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show how our $\\hat{\\boldsymbol  \\theta}$ fits the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\r\n",
    "ax.scatter(x, y);\r\n",
    "xx = [0, 10]\r\n",
    "yy = [0, 10 * theta_hat[0,0]]\r\n",
    "ax.plot(xx, yy, 'red', alpha=.5);\r\n",
    "ax.set(xlabel='x', ylabel='y');\r\n",
    "print(\"theta = %f\" % theta)\r\n",
    "print(\"theta_hat = %f\" % theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we calculate $\\hat{\\boldsymbol  \\theta}$ multiple times, each time taking increasing number of datapoints into consideration. How would you expect  $\\lVert \\hat{\\boldsymbol  \\theta} - \\boldsymbol \\theta \\rVert$ to vary as the number of datapoints increases?\r\n",
    "\r\n",
    "Make your hypothesis, and complete the code below to confirm it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_error = []\r\n",
    "size = []\r\n",
    "\r\n",
    "for i in range(5, 51, 5):\r\n",
    "    # Take the first i points from X and Y\r\n",
    "    X_i = None\r\n",
    "    Y_i = None\r\n",
    "\r\n",
    "    # Calculate theta_hat for X_i and Y_i\r\n",
    "    # Feel free to look at how we did it above in case you are stuck\r\n",
    "    theta_hat = None\r\n",
    "\r\n",
    "    # Removing any excess dimensions from theta_hat\r\n",
    "    theta_hat = np.squeeze(theta_hat)\r\n",
    "\r\n",
    "    # Append the error to the end of theta_error\r\n",
    "    # We have already done this for you\r\n",
    "    theta_error.append(abs(theta - theta_hat))\r\n",
    "    size.append(i)\r\n",
    "\r\n",
    "plt.plot(size, theta_error)\r\n",
    "plt.xlabel(\"dataset size\")\r\n",
    "plt.ylabel(\"parameter error\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, $\\lVert \\hat{\\boldsymbol  \\theta} - \\boldsymbol \\theta \\rVert$ generally decreases with an incrase in the dataset size."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "mathematics-machine-learning-pca",
   "graded_item_id": "5xKMs",
   "launcher_item_id": "Wu0av"
  },
  "interpreter": {
   "hash": "6e43f85b8c899b31f9c6e20303c90261e28e4bc4148d5526e30491a7115eaa38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
