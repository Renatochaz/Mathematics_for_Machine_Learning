{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Imagine we have a data set X which contains n multidimensional vectors:\n",
    "\n",
    "$X = (x_1,...,x_n), x_i \\in \\mathbf{R}^d$ our objective is to find the low representation of the data that is as similar to X as possible.\n",
    "\n",
    "For this objective, let's us write down 3 key aspects behind this:\n",
    "\n",
    "1 - Every vector in $\\mathbf{R}^d$ can be represented as a linear combination of the basis vectors:\n",
    "\n",
    "> $x_n = \\sum_{i=1}^DB_{in}b_i$\n",
    "\n",
    "2 - If we assume we are using an orthonormal basis and the dot product as the inner product:\n",
    "\n",
    ">$B_{in} = x_n^tb$ <- this is the orthogonal projection of $x_n$ on to the one dimensional subspace spanned by the ith basis vectors\n",
    "\n",
    "3 - If we have an orthonomal basis $b_1$ to $b_n$ of $\\mathbf{R}^d$ and we define B to be the matrix tha consists of this orthonormal basis vectors:\n",
    "\n",
    ">$B = (b1,....,b_m)$, the the projection of x on to the subspace can be written as:\n",
    "\n",
    ">$\\tilde{x} = BB^tx$ this is the orthogonal projection of X on to the subspace span by the mth basis vector. and $B^tx$ are the coordinates (or code), which is the coordinates of $\\tilde{x}$ with respect to the basis vectors in the matrix B.\n",
    "\n",
    "The key idea in PCA is to the $\\tilde{x}$ that can be expressed using fewer basis vectors.\n",
    "\n",
    "For this we assume a centered data (mean = 0) and that our basis vectors (b1 to bd) are orthonormal.\n",
    "We can generalize to write:\n",
    "\n",
    "$\\tilde{x}_n = \\sum_{i=1}^m B_{in}b_i + \\sum_{m+1}^D B_{in}b_i$ and in pca we reduce the formula by eliminating the second term, the resulting term is the principal subspace which is spanned by the basis vectors b_1 to b_m\n",
    "\n",
    "> $\\tilde{x}_n = \\sum_{i=1}^m B_{in}b_i$\n",
    "\n",
    "Although $\\tilde{x}_n$ still is a D dimensional vectors, it lives in a m dimensional space.\n",
    "\n",
    "Now, assuming we have data x_1 to x_n, we want to find paramenters $B_{in}$ orthonormal basis vectors $b_i$ such that the average square reconstruction error is minimized. \n",
    "\n",
    "We can write the average squared reconstruction error as:\n",
    "\n",
    "$J = 1/n \\sum_{i=1}^n||x_n-\\tilde{x}_n||^2$\n",
    "\n",
    "Now we find the partial derivates of J in respect to $B_{in}$ and $b_i$, set then to 0 and solve for the optimal parameters. \n",
    "\n",
    "> $\\frac{\\partial J}{\\partial (B_{in},b_i)} = \\frac{\\partial J}{\\partial \\tilde{x}_n}\\frac{\\partial \\tilde{x}_n}{\\partial (B_{in},b_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial J}{\\partial B_{in}} = -2/n(x_n - \\sum_{j=1}^mB_{jn}b_j)^tb = $ assuming Orthonormal basis = $-2/n(x_n^tb - B_{jn}b_j^tb_j)$ = \n",
    "\n",
    "> $-2/n(x_n^tb_j - B_{jn}) = 0$ and this will only be 0 if $x_n^tb_j = B_{jn}$ for $j=1,...,m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use this result now to find our displacement vector:\n",
    "\n",
    "$\\tilde{x_n} = \\sum_{j=1}^mB_{jn}b_j =\\sum_{j=1}^m(x_n^tb_j)b_j = \\sum_{j=1}^mb_j(b_j^tx_n) = (\\sum_{j=1}^mb_jb_j^t)x_n$\n",
    "\n",
    "Note that our projection matrix is $(\\sum_{j=1}^mb_jb_j^t)$, and now writing x_n = \n",
    "\n",
    "$x_n = (\\sum_{j=1}^mb_jb_j^t)x_n + (\\sum_{j=m+1}^Db_jb_j^t)x_n$ and finding the difference:\n",
    "\n",
    "$x_n - \\tilde{x} = (\\sum_{j=m+1}^Db_jb_j^t)x_n$ rearraging:\n",
    "\n",
    "$x_n - \\tilde{x} = (\\sum_{j=m+1}^Db_j^tx_n)b_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use this to reformulate our loss function: $J = 1/n \\sum_{i=1}^n||x_n-\\tilde{x}_n||^2$\n",
    "\n",
    "Using our displacement vector: $J = 1/n \\sum_{i=1}^n|(\\sum_{j=m+1}^Db_j^tx_n)b_j||^2$ =>\n",
    "\n",
    "Now, using our assumption of b_j being orthonormal basis:\n",
    "\n",
    "=> $1/n \\sum_{i=1}^n|(\\sum_{j=m+1}^D(b_j^tx_n)^2 = 1/n\\sum_{i=1}^n|(\\sum_{j=m+1}^Db_j^tx_nx_n^tb_j$ rearranging =>\n",
    "\n",
    "=> $\\sum_{j=m+1}^Db_j^t(1/n\\sum_{i=1}^nx_nx_n^t)b_j$ and we can see that $(1/n\\sum_{i=1}^nx_nx_n^t)$ is the data covariance matrix (because we assumed centered data)\n",
    "\n",
    "now our loss function is:\n",
    "\n",
    "$\\sum_{j=m+1}^Db_j^tSb_j$ with S being our data covariance matrix. (note that this is a inner product)\n",
    "\n",
    "We can also express this function as: $trace((\\sum_{j=m+1}^Db_jb_j^t)S)$ and we can interpret $(\\sum_{j=m+1}^Db_jb_j^t)$ as a projection matrix that takes our data covariance matrix and project it on to the orthogonal complement of the principal subspace.\n",
    "\n",
    "That means that we can see the loss function as the variance of the data projected on to the subspace that we ignore. This means that minimizing the loss is equivalent to minimizing the variance of the data that lies in the subspace that is orthogonal to the principal subspace. \n",
    "\n",
    "We want to retain as much variance after projection as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us do a quick recap to have an easy to access summary of our principal formulas in PCA:\n",
    "    \n",
    "1 - $\\tilde{x}_n = \\sum_{j=1}^mB_{jn}b_j$\n",
    "\n",
    "2 - $J = 1/n \\sum_{i=1}^n||x_n-\\tilde{x}_n||^2$ (loss function)\n",
    "\n",
    "3 - $\\frac{\\partial J}{\\partial \\tilde{x}_n} = -2/N(x_n-\\tilde{x}_n)^t$\n",
    "\n",
    "4 - ${B_jn} = x_n^tb_j, j = 1,..,m$\n",
    "\n",
    "5 - $x_n - \\tilde{x} = (\\sum_{j=m+1}^Db_j^tx_n)b_j$ (displacement vector)\n",
    "\n",
    "6 - $ J = \\sum_{j=m+1}^Db_j^tSb_j$ (loss function reformulated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we want to do is to minimize $\\sum_{j=m+1}^Db_j^tSb_j$, for this objective we need to find the orthonormal basis that spans the subspace we will ignore. \n",
    "\n",
    "When we have that basis, we take its orthogonal complement as the basis of the principal subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the orthogonal complement of a subspace U consists of all vectors of the original vector space that are orthogonal to every vector in U."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the lagranger multipler we can find that J is minimized when $J = \\lambda$, and that means that we need to chose b_2 (in a 2d example, when we are minimizing to 1 dimension) as the corresponding eigenvector and that one will span the subspace we will ignore.\n",
    "b_1 then which spans the principal subspace is the eigenvector that belongs to the largest eigenvalue of the data covariance matrix.\n",
    "\n",
    "If we look at this 2-dimensional example, the best projection we can have (that retains most information) is the one that projects onto the subspace that is spanned by the eigenvector of the data covariance matrix which belongs to the largest eigenvalue.\n",
    "\n",
    "Now, for a general case, if we want to find the m dimensional subspace of a d dimensional dataset, and we solve for the basis vectors:\n",
    "\n",
    "$b_j, j = m+1,...D$, we end up with: $Sb_j = \\lambda_jb_j, j=m+1,..,D$, then:\n",
    "\n",
    "$J = \\sum_{j=m+1}^D\\lambda_j$ this means that for the general case, the average reconstruction error is minimized if we choose the basis vectors that spanned the ignored subspace to be the eigenvectors of the data covariance matrix that belongs to the smallest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the eigenvectors of the covariance matrix are orthogonal, and the eigenvector belonging to the largest eigenvalue points in the direction of the data with the largest variance, and the variance in that direction is given by the eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to implement PCA are as follows:\n",
    "\n",
    "Substract the mean to the data to make it centralized and avoid numerical problems;\n",
    "\n",
    "Divide the datapoints by the std to make it unit free;\n",
    "\n",
    "Compute the eigenvalues and eigenvectos of the data covariance matrix;\n",
    "\n",
    "Project any datapoing on to the principal subspace that are spanned by the eigenvectors that belongs to the largest eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal note\n",
    "\n",
    "To implement PCA (without std. scaling) and in python:\n",
    "\n",
    "1 - Create a function that compute the mean of each dimension (i.e columns of dataset), note that for this you will need to select axis = 0 for the numpy mean argument. **Note that the function in the assigment you will do this and center the dataset in the same function, so you can just apply this function as the whole step 1 in the pca algorithm**\n",
    "\n",
    "2 - Create a function ta computes eigenvalues ang eigenvectors (pretty simple with np.linalg.eig), then sort the eigenvectors columns to a descending order, so each eigenvector starts with the highest value.\n",
    "\n",
    "3 - Compute the projection matrix, our general formula from week 3 is: $B(B^tB)^{-1}B^t$, with B being our basis for the subspace\n",
    "\n",
    "4 - Now for the PCA algorithm: \n",
    "\n",
    "> Center our dataset (X) with the mean calculated from function 1;\n",
    "\n",
    "> Compute covariance matrix of this normalized dataset (dont forget the options rowvar = false and bias = true);\n",
    "\n",
    "> Find the eigenvalues and eigenvectors with the function 2;\n",
    "\n",
    "> Select and assign the eigenvalues and eigenvectors equivalent to the number of principal components (note that you will need to use the colon(:) operator to select the right argument, a tip: X[,:number]\n",
    "\n",
    "> Assign your projection matrix using the principal components (the eigenvectors you selected previously, they are in a variable, right?)\n",
    "\n",
    "> reconstruct your data using your normalized dataset dotted with the projection matrix (and add the mean after this!)\n",
    "\n",
    "> Done! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
