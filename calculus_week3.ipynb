{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate chain rule\n",
    "\n",
    "Remember our formula to find the total derivative of a function with three variables:\n",
    "\n",
    "$\\frac{\\partial f(x,y,z)}{\\partial t} = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial t} + \\frac{\\partial f}{\\partial z}\\frac{\\partial z}{\\partial t}$\n",
    "\n",
    "Now let us generalize this concept to a more computational approach:\n",
    "\n",
    "Given the function f(x1,x2,...,xn), we can write this function as: f(x), with x being a n-dimensional vector.\n",
    "\n",
    "So if we want to find the total derivative of f(x) in relation to another scalar (t), we need to compute the following vectors:\n",
    "\n",
    "derivative of f in relation to each x:\n",
    "\n",
    "$\\frac{\\partial f(x)}{\\partial x} = \\begin{bmatrix} \\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ . \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}$\n",
    "\n",
    "and derivative of each x in relation to t:\n",
    "\n",
    "$\\frac{\\partial x}{\\partial t} = \\begin{bmatrix} \\frac{\\partial x_1}{\\partial t} \\\\ \\frac{\\partial x_2}{\\partial t} \\\\ . \\\\ \\frac{\\partial x_n}{\\partial t} \\end{bmatrix}$\n",
    "\n",
    "And now to find our total derivative we can use the dot product between those vectors:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial t} = \\frac{\\partial f(x)}{\\partial x} . \\frac{\\partial x}{\\partial t} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our Jacobian vector? did you notice that its the same as our $\\frac{\\partial f(x)}{\\partial x}$ but with a row form instead of column?\n",
    "\n",
    "You can express this by saying that the jacobian is equal to our partial f(x) in its transposed form:\n",
    "\n",
    "$(J_f)^t = \\frac{\\partial f(x)}{\\partial x}$\n",
    "\n",
    "But more importantly, we can simplify the multivariate chain rule with the jacobian vector, since the multiplication of a row vector by a column vector is the same operation as the dot product:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial t} = J_f \\frac{\\partial x}{\\partial t} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize how to construct the chain rule for a simple case, and you can use this knowledge to later on solve more complicated problems (which i hope you are doing in a computer and not by hand!)\n",
    "\n",
    "How do you derivate $\\frac{df}{dt}$ if this is your function: $f(x(u(t)))$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start looking at we got in your hands:\n",
    "\n",
    "$f(x) = f(x_1,x_2)$\n",
    "\n",
    "$x(u) = \\begin{bmatrix} x_1(u_1,u_2) \\\\ x_2(u_1,u_2)\\end{bmatrix}$\n",
    "\n",
    "$u(t) = \\begin{bmatrix} u_1(t) \\\\ u_2(t) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's construct our chain rule:\n",
    "\n",
    "$\\frac{df}{dt} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial u} \\frac{\\partial u}{\\partial t}$\n",
    "\n",
    "Now we are getting somewhere! If we already have the notation, we just need to understand what each partial represents:\n",
    "\n",
    "$\\frac{df}{dt} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial u} \\frac{\\partial u}{\\partial t} = [\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2}] ... \\begin{bmatrix} \\frac{\\partial u_1}{\\partial t} \\\\ \\frac{\\partial u_2}{\\partial t} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty straightforward, but what about our middle term(...) which represents the partial of x to u? Well, we need to derivate each term (x1 and x2) by each u(u1 and u2), and this gives us the final expression:\n",
    "\n",
    "$\\frac{df}{dt} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial u} \\frac{\\partial u}{\\partial t} = [\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2}] \\begin{bmatrix} \\frac{\\partial x_1}{\\partial u_1} \\frac{\\partial x_1}{\\partial u_2} \\\\ \\frac{\\partial x_2}{\\partial u_1} \\frac{\\partial x_2}{\\partial u_2} \\end{bmatrix}\\begin{bmatrix} \\frac{\\partial u_1}{\\partial t} \\\\ \\frac{\\partial u_2}{\\partial t} \\end{bmatrix}$\n",
    "\n",
    "Now we just need to calculate the partial derivates and do the maths!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network in matrix format\n",
    "\n",
    "Here we are going to visualize what happens beneath a neural network in its matrix format.\n",
    "\n",
    "Since no image will be avaliable to plot here, this part can be difficult to understand at first, but the maths are solid and can be usefull for those who already have a visual understanding a neural network:\n",
    "\n",
    "> first exemple with a single neural network \n",
    "\n",
    "> this network takes a single scalar (input) $a^{(0)}$ and returns another scalar(output) $a^{(1)}$\n",
    "\n",
    "We can write this function as: $a^{(1)} = \\sigma(wa^{(0)} + b)$\n",
    "\n",
    "a = activity\n",
    "w = weight\n",
    "b = bias\n",
    "$\\sigma$ = activate function\n",
    "\n",
    "Note that sigma is where the name \"neural networks\" comes from, since it \"copy\" a neuron: When sigma exceeds a threshold, it starts stimulating the \"neuron\" and the function is activated.\n",
    "\n",
    "> Now we are going to extendend our model, we will add another neuron that is function of $a^{(1)}$, so now we have two neurons:\n",
    "\n",
    "$a_0^{(0)}$ and $a_1^{(0)}$ and both of those returns another scalar: $a^{(1)}$\n",
    "\n",
    "Now to add this in our math notation we simple say that $a^{(1)}$ is equal to sigma times the sum of those neurons plus the bias:\n",
    "\n",
    "$a^{(1)} = \\sigma(w_0a_0^{(0)} + w_1a_1^{(0)} + b)$\n",
    "\n",
    "if we add another neuron (or as many as we like), we simple follow the same logic:\n",
    "\n",
    "$a^{(1)} = \\sigma(w_0a_0^{(0)} + w_1a_1^{(0)} + w_2a_2^{(0)} + b)$\n",
    "\n",
    "Note that we can generalize this expression with the summation notation:\n",
    "\n",
    "$a^{(1)} = \\sigma( (\\sum_{j=0}^n w_ja_j^{0}) + b)$\n",
    "\n",
    "but note! each input has a correspong weight, so we can make a vector if inputs and a vector of weights and take their dot product:\n",
    "\n",
    "$a^{(1)} = \\sigma (\\hat{w}.\\hat{a_{0}} + b)$\n",
    "\n",
    "now that we can add as many inputs as we like, let's finally extend for as many output as we like:\n",
    "\n",
    "> now we have 3 inputs ($a_0^{(0)}, a_1^{(0)}, a_2^{(0)})$ and 2 outputs ($a_0^{(1)}, a_1^{(1)}$)\n",
    "\n",
    "Now each output will have its own formula with its weight and bias:\n",
    "\n",
    "$a_0^{(1)} = \\sigma (\\hat{w_0}.\\hat{a_{0}} + b_0)$\n",
    "\n",
    "$a_1^{(1)} = \\sigma (\\hat{w_1}.\\hat{a_{0}} + b_1)$\n",
    "\n",
    "each equation for an output has the same inputs (and sigma!) but with different bias and weights\n",
    "\n",
    "and now finally we write this in a compact way using matrix notation:\n",
    "\n",
    "$a^{(1)} = \\sigma (W^{(0)}.\\hat{a_{0}} + b^{(1)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our vectors w are now represented as a matrix, our bias is now represented as a vector and our outputs are represented as columns vectors, each position represents an output.\n",
    "\n",
    "Now we extended our single layer neural network to m-1 outputs (since it starts with 0) and n-1 inputs, and the formula remains the same:\n",
    "\n",
    "$a^{(1)} = \\sigma (W^{(0)}.\\hat{a_{0}} + b^{(1)})$\n",
    "\n",
    "but our formula can be seen in its matrix format as:\n",
    "\n",
    "$\\begin{bmatrix} a_0^{(1)} \\\\ a_1^{(1)} \\\\ . \\\\ a_{m-1}^{(1)} \\end{bmatrix} = \\sigma \\left ( \\begin{bmatrix} w_{(0,0)}^{(1)} & w_{(0,1)}^{(1)} & ... & w_{(0,n-1)}^{(1)} \\\\ w_{(1,0)}^{(1)} & w_{(1,1)}^{(1)} & ... & w_{(1,n-1)}^{(1)} \\\\ . & . & . & . \\\\ w_{(m-1,0)}^{(1)} & w_{(m-1,1)}^{(1)} & ... & w_{(m-1,n-1)}^{(1)} \\end{bmatrix} . \\begin{bmatrix} a_0^{(0)} \\\\ a_1^{(0)} \\\\ . \\\\ a_{n-1}^{(0)} \\end{bmatrix} + \\begin{bmatrix} b_0^{(1)} \\\\ b_1^{(1)} \\\\ . \\\\ b_{m-1}^{(1)} \\end{bmatrix}\\right )$\n",
    "\n",
    "With this we have our final formula for a feedforward neural network with as many layers of inputs and outputs:\n",
    "\n",
    "$a^{(L)} = \\sigma (W^{(L)}.\\hat{a}^{L-1} + b^{(L)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network applied with the chain rule\n",
    "\n",
    "The classic method to train a network is to do a backpropagation.\n",
    "\n",
    "When give some input, we want this input to pass thorugh our network (with all weight and bias) and be as close as possible to our output.\n",
    "\n",
    "To achieve this, we first pass a random set of weight and bias, and we will get a certain output.\n",
    "\n",
    "And how can we fine tune this weight and bias to give us the best possible output? We simple define a Cost function (C):\n",
    "\n",
    "$C = \\sum_i(a_i^{(L)} - y_i)^2$\n",
    "\n",
    "witch is simple the sum of the square of the difference between the desired output (y) and the output our untrained network gives us (a)\n",
    "\n",
    "Now what do we do? We minimize this Cost function C! When this function is at is minima, it means our network is givings us the output closest to the one we desire (y)\n",
    "\n",
    "As we seen before, we can head towards the minima point of a multimendion system with the jacobian, with the derivate of the cost function in respect to all relevant variables.\n",
    "\n",
    "Imagine a network with one input and one output:\n",
    "\n",
    "$a^{(1)} = \\sigma(wa^{(0)} + b)$\n",
    "\n",
    "$C = (a^{(1)} - y)^2$\n",
    "\n",
    "Our C derivatives would be in respect to w and b (note the chain rule here):\n",
    "\n",
    "$\\frac{dC}{dw} = \\frac{\\partial C}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial w}$\n",
    "\n",
    "$\\frac{dC}{db} = \\frac{\\partial C}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial b}$\n",
    "\n",
    "Note that it' often convenient to add one more function to our chain rule expression to navigate between our minima points of C in respect to w and b:\n",
    "\n",
    "$z^{(1)} = wa^{(0)} + b$, with this our equation becomes:\n",
    "\n",
    "$a^{(1)} = \\sigma z^{(1)})$, and our partial derivatives now need to include our z term:\n",
    "\n",
    "$\\frac{dC}{dw} = \\frac{\\partial C}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial w}$\n",
    "\n",
    "$\\frac{dC}{db} = \\frac{\\partial C}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything will becomer harder from now on, with our generalized form of what has been exemplified here, but the logic is the same:\n",
    "\n",
    "$z^{(L)} = W^L . a^{(L-1)} + b^L$\n",
    "\n",
    "$a^{(L)} = \\sigma(z^{(L)})$\n",
    "\n",
    "$C = \\sum_i(a_i^{(L)} - y_i)^2$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
