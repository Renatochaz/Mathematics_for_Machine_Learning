{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einsten summation, notes on matrix multiplication and symmetry of the dot product\n",
    "\n",
    "When multiplying two matrices, like $A_{ij}$ and $B_{jk}$:\n",
    "\n",
    "$\\begin{bmatrix} a_{11} & a_{12} & . & a_{1j} \\\\ a_{21} & a{22} & . & a_{2j} \\\\ . & . & . & . \\\\ a_{i1} &a_{i2} & . & a_{ij}\n",
    "\\end{bmatrix}$ $\\times$ $\\begin{bmatrix} b_{11} & b_{12} & . & b_{1k} \\\\ b_{21} & b_{22} & . & b_{2k} \\\\ . & . & . & . \\\\ b_{j1} & b_{j2} & . & b_{jk}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The first index number of the elements represent the row, and the second the column, so the element $a_{12}$ represents element a in the first row and second column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we multiply those two matrix, we get a new matrix (call it C) with i rows and k columns: $C_{ik}$,\n",
    "we can describe the multiplcation in the einstein summation: $A_{ij} B_{jk}$ (leaving the sigma from the summation notation out). \n",
    "\n",
    "If we want the element $C_{ik}$, we can computate: $C_{ik} = A_{ij} B_{jk}$ \n",
    "\n",
    "- Note that when multiplying it is indispensable that the right hand side matrix have the same number of rows ($j$) as the number of columns of the left side matrix (also $j$), then the resultant matrix (C) will have the same number of rows of the left hand side matrix ($i$) and the same number of columns of the right hand side matrix ($k$).\n",
    "\n",
    "Example: For $C_{23}$, we can computate: $C_{23} = A_{2j} B_{j3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice exercises - Non-square matrix multiplication\n",
    "\n",
    "### 1 -\n",
    "\n",
    "Pergunta 1\n",
    "In the previous lecture we saw the Einstein summation convention, in which we sum over any indices which are repeated. In traditional notation we might write, for example, $\\sum_{j=1}^3 A_{ij}v_{j} = A_{i1}v_{1} +A_{i2}v_{2}+A_{i3}v_{3}$. With the Einstein summation convention we can avoid the big sigma and write this as $A_{ij}v_{j}$. We know that we sum over $j$ because it appears twice.\n",
    "\n",
    "We saw that thinking about this type of notation helps us to multiply non-square matrices together. For example, consider the matrices\n",
    "\n",
    "$A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 0 & 1\\end{bmatrix}$ and$ B = \\begin{bmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 0 & 1\\end{bmatrix}$ \n",
    "and remember that in the $A_{ij}$ notation the first index $i$ represents the row number and the second index $j$ represents the column number. For example, $A_{12} = 2$ \n",
    "\n",
    "Let's define the matrix $C = ABC=AB$. Then in Einstein summation convention notation $C_{mn} = A_{mj}B_{jn}$\n",
    "Using the Einstein summation convention, calculate $C_{21} = A_{2j}B_{j1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C_{21} = (4*1) + (0*0) + (1*1) = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "\n",
    "Calculate the product:\n",
    "\n",
    "$\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ $\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(1*1) + (0 * 4) = 1$(element $a_{11}$)\n",
    "\n",
    "$(1*2) + (0 * 5) = 2$(element $a_{12}$)\n",
    "\n",
    "$(1*3) + (0 * 6) = 3$(element $a_{13}$)\n",
    "\n",
    "$(0*1) + (1 * 4) = 4$(element $a_{11}$)\n",
    "\n",
    "$(0*2) + (1 * 5) = 5$(element $a_{12}$)\n",
    "\n",
    "$(0*3) + (1 * 6) = 6$(element $a_{13}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New matrix:\n",
    "\n",
    "$\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice exercises: Using non-square matrices to do a projection\n",
    "\n",
    "### 1 - \n",
    "The sun is sufficiently far away that effectively all of its rays come in parallel to each other. \n",
    "\n",
    "We can describe their direction with the unit vector $\\hat{\\mathbf{s}}$. \n",
    "\n",
    "We can describe the 3D coordinates of points on objects in our space with the vector $\\mathbf{r}$. \n",
    "\n",
    "Objects will cast a shadow on the ground at the point $\\mathbf{r}'$ along the path that light would have taken if it hadn't been blocked at $\\mathbf{r}$, that is, $\\mathbf{r}' = \\mathbf{r} + \\lambda \\hat{\\mathbf{s}}$. \n",
    "\n",
    "The ground is at $\\mathbf{r}'_3 = 0$ by using $\\mathbf{r}'.\\hat{\\mathbf{e}}_3 = 0$, we can derive the expression, $\\mathbf{r}.\\hat{\\mathbf{e}}_3 + \\lambda s_3 = 0$ (where $s_3 = \\hat{\\mathbf{s}}.\\hat{\\mathbf{e}}_3$). \n",
    "\n",
    "Rearrange this expression for $\\lambda$ and substitute it back into the expression for $\\mathbf{r}'$ in order to get $\\mathbf{r}'$ in terms of $\\mathbf{r}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{r}.\\hat{\\mathbf{e}}_3 + \\lambda s_3 = 0$\n",
    "\n",
    "Solving for $\\lambda$\n",
    "\n",
    "> $\\lambda s_3 = - \\mathbf{r}.\\hat{\\mathbf{e}}_3$\n",
    "\n",
    "> $\\lambda = - \\mathbf{r}.\\hat{\\mathbf{e}}_3/s_3$\n",
    "\n",
    "Subtituting solved $\\lambda$ in expression: $\\mathbf{r}' = \\mathbf{r} + \\lambda \\hat{\\mathbf{s}}$. \n",
    "\n",
    "> $\\mathbf{r}' = r + \\hat{\\mathbf{s}} (- \\mathbf{r}.\\hat{\\mathbf{e}}_3/s_3)$\n",
    "\n",
    "> $\\mathbf{r}' = r - \\hat{\\mathbf{s}} (\\mathbf{r}.\\hat{\\mathbf{e}}_3)/s_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - \n",
    "From your answer above, you should see that $\\mathbf{r}'$ can be written as a linear transformation of $\\mathbf{r}$. This means we should be able to write $\\mathbf{r}' = A \\mathbf{r}$ for some matrix A.\n",
    "\n",
    "To help us find an expression for A, we can re-write the expression above with Einstein summation convention.\n",
    "\n",
    "Which of the answers below correspond to the answer to Question 1? (Select all that apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us change all of our vectors to their indexed formats, that is:\n",
    "\n",
    "$\\mathbf{r}' = r - \\hat{\\mathbf{s}} (\\mathbf{r}.\\hat{\\mathbf{e}}_3)/s_3$ =\n",
    "\n",
    "> $\\mathbf{r_i}' = r_i - {\\mathbf{s_i}} (\\mathbf{r_i}.\\hat{\\mathbf{e}}_3j)/s_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it becomes easy to find the solutions, such as : $\\mathbf{r_i}' = r_i - {\\mathbf{s_i}} \\mathbf{r_3}/s_3$\n",
    "See if you can find all the right solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 -\n",
    "\n",
    "Based on your answer to the previous question, or otherwise, you should now be able to give an expression for A in its component form by evaluating the components $A_{ij}$ for each row i and column j.\n",
    "\n",
    "Since A will take a 3D vector, $\\mathbf{r}$, and transform it into a 2D vector, $\\mathbf{r}'$, we only need to write the first two rows of A. That is, A will be a 2Ã—3 matrix. Remember, the columns of a matrix are the vectors in the new space that the unit vectors of the old space transform to - and in our new space, our vectors will be 2D.\n",
    "\n",
    "What is the value of A?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is a little trick, so let's go step by step:\n",
    "\n",
    "We know from question 1 that the expression for r' is: > $\\mathbf{r}' = r - \\hat{\\mathbf{s}} (\\mathbf{r}.\\hat{\\mathbf{e}}_3)/s_3$ (1)\n",
    "\n",
    "and know from question 2 that: $\\mathbf{r}' = A \\mathbf{r}$ (2)\n",
    "\n",
    "So let us first find what exactly is r' in its matrix form so we can visualize the expression (2) in another light:\n",
    "\n",
    "$\\mathbf{r}' = r - \\hat{\\mathbf{s}} (\\mathbf{r}.\\hat{\\mathbf{e}}_3)/s_3$  = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We know that r is a 3d vector, which means that r is a vector with dimension 3: $\\begin{bmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{bmatrix}$ and that s is another 3 dimensional vector:  $\\begin{bmatrix} s_1 \\\\ s_2 \\\\ s_3 \\end{bmatrix}$\n",
    "\n",
    "We also that $\\mathbf{r}.\\hat{\\mathbf{e}}_3$ = $r_3$\n",
    "\n",
    "Then substitute expression (1) using this information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{r}' = \\begin{bmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{bmatrix} - \\begin{bmatrix} s_1 \\\\ s_2 \\\\ s_3 \\end{bmatrix} \\times r_3/s_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this gives us: \n",
    "\n",
    "$\\mathbf{r}' = \\begin{bmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{bmatrix} - \\begin{bmatrix} s_1r_3/s_3 \\\\ s_2r_3/s_3 \\\\ s_3r_3/s_3 \\end{bmatrix}$\n",
    "\n",
    "now simplify the last expression of vector s to get:\n",
    "\n",
    "$\\mathbf{r}' = \\begin{bmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{bmatrix} - \\begin{bmatrix} s_1r_3/s_3 \\\\ s_2r_3/s_3 \\\\ r_3 \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally:\n",
    "\n",
    "$\\mathbf{r}' = \\begin{bmatrix} r_1-(s_1r_3/s_3) \\\\ r_2-(s_2r_3/s_3) \\\\ 0 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we substitute this expression for r' in equation 2 to get:\n",
    "\n",
    "$\\begin{bmatrix} r_1-(s_1r_3/s_3) \\\\ r_2-(s_2r_3/s_3) \\\\ 0 \\end{bmatrix} = A \\times \\begin{bmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly note that vector r' is a vector 3x1 and r is a vector 3x1, so we need A to be a matrix 3x3 to be able to multiply r and gives us a 3x1 vector.\n",
    "\n",
    "So we just need to visualize the multiplication of A that will give us the right-hand side expression, which is:\n",
    "\n",
    "$\\begin{bmatrix}1 & 0 & -s_1 / s_3 \\\\ 0 & 1 & -s_2 / s_3 \\\\ 0 & 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "- note that the exercise only give us the option: $\\begin{bmatrix}1 & 0 & -s_1 / s_3 \\\\ 0 & 1 & -s_2 / s_3\\end{bmatrix}$ since the last row is 0, but the actual result is the 3x3 matrix with 0 in its final rows (we use that information for question 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices changing basis\n",
    "\n",
    "If you have the basis vector of another system (call it beta) in the coordinates you are using (call it origin), the multiplication of those basis vectors (from beta system) $\\times$ the vectors in the beta system, will result in the vectors translated to the origin system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize it would be like this:\n",
    "\n",
    "Suppose beta basis vectors translated to the origin system are B, and the vectors in beta system are r. Their result would be vector v (in the origin system)\n",
    "\n",
    "$B \\times r = v$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inverse proccess, if you want to translate one vector from the origin system to the beta system, you just need the inverse of the beta basis vectors translated to the origin: $B^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the inverse, you just multiply the vectors in the origin system with $B^{-1}$ and get the vector in the beta system, like this:\n",
    "\n",
    "$B^{-1} \\times v = r$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For projection you can use the dot product, only if the basis are orthogonal.\n",
    "\n",
    "You can simply multiply (dot product) the vector from system A times the axis of the basis system B (translated to the system A) to get the vector in the system B, instead of using the inverse of system B translated basis.\n",
    "\n",
    "### Transformation in a changed basis\n",
    "\n",
    "If you want to do a transformation in a different basis system, you can write down the transformation in the origin system (the basic 1,0 and 0,1 system).\n",
    "\n",
    "Lets call the matrix that do the transformation T.\n",
    "Then you multiply T by the basis vector of the other system translated to the origin (The B we discussed earlier), like this:\n",
    "$T \\times B$ = F.\n",
    "This gives you the transformation matrix in the origin system (F), now we just multiply F by $B^{-1}$ (the invere of B) and we get the transformation matrix in the new system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal matrices\n",
    "\n",
    "In data science you always want to use a orthonomal basis vector set  when transforming the data, since the inverse is easilly computed, the transformation is reversible, the projection is the dot product, the determinant will be 1 (or -1) and this will make all the computation easy and faster.\n",
    "\n",
    "The orthogonal matrices have the following properties:\n",
    "\n",
    "- The inverse matrix is the tranpose ($A^tA = I$)\n",
    "- Determinant of an orthogonal matrix is 1 or -1\n",
    "- An orthogonal matrix is compose of rows (and columns) of orthogonal vectors .\n",
    "- In a orthogonal matrix: $a_i a_j = 0$ if $i \\neq j$ and $a_i a_j = 1$ if $i = j$\n",
    "- The transpose of a orthonomal basis vector set is itself another orthogonal basis vector set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gram-schmidt process\n",
    "\n",
    "Creating an orthonomal basis vector set.\n",
    "\n",
    "If we have linearly independent vectors: \n",
    "\n",
    "v = {v1,v2,...,vn}\n",
    "\n",
    "first we compute our first axis normalizing v1 by its length: $e_1 = v_1/|v_1|$\n",
    "\n",
    "Then we will compose normal projection of the other vectors, like this:\n",
    "\n",
    "$u_2 = v_2 - (v_2 e_1)e_1$ and we normalize those projects by its length: $e_2 = v_2/|v_2|$\n",
    "\n",
    "For $v_3$ (and subsequent vectors) we use the same method and keep doing compositions of our orthonomal vectors:\n",
    "\n",
    "$u_3 = v_3 - (v3 e_1)e_1 - (v_3 e_2)e_2$ and normalize: $e_3 = u_3/|u_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
